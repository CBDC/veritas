{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from IPython.display import HTML\n",
    "\n",
    "# HTML('''<script>\n",
    "# code_show=true; \n",
    "# function code_toggle() {\n",
    "#  if (code_show){\n",
    "#  $('div.input').hide();\n",
    "#  } else {\n",
    "#  $('div.input').show();\n",
    "#  }\n",
    "#  code_show = !code_show\n",
    "# } \n",
    "# $( document ).ready(code_toggle);\n",
    "# </script>\n",
    "# <form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing spectra from VERITAS database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will structure the pipeline to process the publicly available\n",
    "[VERITAS blazars spectra][VDB].\n",
    "The goal is to collect the (ascii) data files at the bottom [that page][VDB],\n",
    "transform and structure them to VO-compliant data structures.\n",
    "Also, flux data should be standardized to [CGS][CGS] units $erg.s^{-1}.cm^{-2}$,\n",
    "and wavelength/frequency to $Hz$.\n",
    "\n",
    "[VDB]: http://veritas.sao.arizona.edu/veritas-science/veritas-blazar-spectra\n",
    "[CGS]: https://en.wikipedia.org/wiki/Centimetre%E2%80%93gram%E2%80%93second_system_of_units\n",
    "\n",
    "The table of interest at the [VERITAS page][VDB] is the last one on that page -- which we'll call here\n",
    "*target table* --, where we have the columns:\n",
    "\n",
    "| Blazar | Publication | VERITAS results page | Ascii file with spectral data |\n",
    "|--------|-------------|----------------------|-------------------------------|\n",
    "| ...    | ...         | ...                  | ...                           |\n",
    "\n",
    "Where, \n",
    "* `Blazar` contains the name of the object and possibly a qualifier about blazar state of activity in parenthesis after the name (as the example). This column is *not* unique!\n",
    "* `Publication` is neither an unique column and contains the identifier of the article where the respective data was published; this is *not* a link or a standard (e.g, DOI) value.\n",
    "* `VERITAS results page` contains links to another (internal) page where plots of the article are provided; the link to the `ArXiv` version of the article is there provided\n",
    "* `Ascii file with spectral data` are links to the data table.\n",
    "\n",
    "The ASCII files have the following structure (follow the head line of those files):\n",
    "```\n",
    "    //VHE points: E[TeV] phi[m-2 s-1 TeV-1] ephi_low ephi_up\n",
    "```\n",
    "\n",
    "All files have the same head line, which means that all tables have the same metadata, which means that column order and units are all the same!\n",
    "\n",
    "**AlreadyGood**:\n",
    "* data files have all the same structure, same metadata, same units;\n",
    "\n",
    "**ToImprove**:\n",
    "* `Blazars` column should contain *only* the object name, o//VHE points: E[TeV] phi[m-2 s-1 TeV-1] ephi_low ephi_upbservations like the object's state of activity (e.g, high/low) are better to go in another column;\n",
    "* `MJD` can be added as an extra column;\n",
    "* `Publication` could be label as it is, but a hyperlink to ArXiv;\n",
    "\n",
    "<div style=\"margin-top:30px; margin-bottom:30px;\n",
    "            background-color:orange; color:black; opacity:0.75; padding:10px;\">\n",
    "    Each data file is assumed to be the result of <b>one</b>, contiguous observation.\n",
    "</div>\n",
    "\n",
    "\n",
    "The final result we are looking for is a master table with:\n",
    "\n",
    "| Blazar name | ICRS position (x2) | MJD (x2) | Publication (hyperlink) | Data file (FITS) | Note |\n",
    "|-------------|--------------------|----------|-------------------------|------------------|------|\n",
    "| ...         | (RA & Dec)         | (Start & End) | (ArXiv)            | ...              | ...  |\n",
    "\n",
    "\n",
    "The `ICRS position` is **not** provided, we have to query [Simbad][SIMBAD] for it.\n",
    "\n",
    "`MJD` (epoch) is not directly provided either, but we should probably get them from the other\n",
    "tables at the [same page][VDB], or create another, local table from the articles.\n",
    "\n",
    "`Publication` (the ArXiv) link should be parsed from the related page provided by the\n",
    "*target table*'s `Publication` column.\n",
    "\n",
    "Finally, `Blazar name` and `Note` is already given and `Data file` will be the result of inputs formatting.\n",
    "\n",
    "[SIMBAD]: http://simbad.u-strasbg.fr/simbad/\n",
    "\n",
    "The results of this processing are publicly available through a [Virtual Observatory (VO)][IVOA] \n",
    "compatible framework at the [Brazilian Science Data Center (BSDC)][BSDCVO].\n",
    "A [web interface][BSDCWEB] and a [SSAP service][BSDCSAP] are provided for accesing the data.\n",
    "\n",
    "[IVOA]: http://www.ivoa.net/\n",
    "[BSDCVO]: http://vo.bsdc.icranet.org/\n",
    "[BSDCWEB]: http://vo.bsdc.icranet.org/veritas/q/web/form\n",
    "[BSDCSAP]: http://vo.bsdc.icranet.org/veritas/q/ssa/info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The workflow\n",
    "\n",
    "Here goes the workflow we shoud code below:\n",
    "\n",
    "1. Retrieve the 'target table' from 'http://veritas.sao.arizona.edu/veritas-science/veritas-blazar-spectra'\n",
    "1. Parse each line onto\n",
    " * object name\n",
    " * note\n",
    " * publication \"bibcode\"\n",
    " * results url\n",
    " * data url\n",
    " * **Process data file**\n",
    "1. Create the output table\n",
    "\n",
    "*Process data file*:\n",
    "1. retrieve ICRS position from object name\n",
    "1. parse results page to get publication's ArXiv link\n",
    "1. download ascii data file\n",
    " * transform first column, `E`, from $TeV$ to $Hz$\n",
    " * transform second column, `phi`, from $m^{-2} s^{-1} TeV^{-1}$ to $erg s^{-1} cm^{-2}$\n",
    " * transform 3rd and 4st columns, `ephi_low` and `ephi_up`, as done for `phi`\n",
    " * associate those columns with proper UCDs\n",
    "1. write everything in a FITS file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "veritas_url = 'http://veritas.sao.arizona.edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# first, set the display width for a smaller number than the deafult (80)\n",
    "pandas.set_option('display.width', 50)\n",
    "\n",
    "# print one column at a time (\"per line\")\n",
    "pandas.set_option('display.max_colwidth', pandas.get_option('display.width') )\n",
    "\n",
    "# colmuns' header justified at the left, next to index\n",
    "# pandas.set_option('display.colheader_justify', 'left')\n",
    "\n",
    "# limit the number or rows to just a few\n",
    "pandas.set_option('display.max_rows', 10)\n",
    "\n",
    "# import wget\n",
    "# import bs4\n",
    "\n",
    "def eprint(string):\n",
    "#     from sys import stderr\n",
    "#     print >> stderr, \"{}\".format(string)\n",
    "#     stderr.flush()\n",
    "    print \"\\nERROR:{}\".format(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HTMLBase(object):\n",
    "    fields = {}\n",
    "\n",
    "    def __init__(self,html):\n",
    "        self._html = html\n",
    "\n",
    "    def html(self):\n",
    "        return self._html\n",
    "\n",
    "    def extract_fields(self):\n",
    "        assert False, \"Not implemented. This is a base class.\"\n",
    "\n",
    "        \n",
    "def check_table_header(row,fields):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cells = row.findAll('td')\n",
    "    for i,cell in enumerate(cells):\n",
    "        txt = cell.find(text=True)\n",
    "        txt = txt.strip()\n",
    "        if txt in fields.keys():\n",
    "            fields[txt] = i\n",
    "    return all([ v is not None for v in fields.values() ])\n",
    "\n",
    "\n",
    "def process_row(row,fields,get_doi=False):\n",
    "    cells = row.findAll('td')\n",
    "    if len(cells)==4:\n",
    "        # Object source name(s) (can be more then one comma separated)\n",
    "        _i_ = fields['Blazar']\n",
    "        src = cells[_i_].find(text=True)\n",
    "        src = src.strip()\n",
    "        # Article reference (url), usually a ref to ads\n",
    "        _i_ = fields['VERITAS results page']\n",
    "        art = cells[_i_].find('a',href=True)\n",
    "        url = art['href']\n",
    "        if get_doi:\n",
    "            url = get_doi_url(url)\n",
    "        # We skip year of publication (third column)\n",
    "        # as well as bibcode reference (fourth column)\n",
    "        #ref = cells[3].find(text=True).encode('utf8')\n",
    "        # FITS file link for downloading it in the near future\n",
    "        _i_ = fields['Ascii file with spectral data']\n",
    "        fits = cells[_i_]\n",
    "        ffile = fits.find('a',href=True)\n",
    "        try:\n",
    "            ffile = ffile['href']\n",
    "        except:\n",
    "            ffile = None\n",
    "        #furl = url+ffile if ffile!=None else '_NULL_'\n",
    "        return (src,url,ffile)\n",
    "    return None\n",
    "\n",
    "\n",
    "class HTMLVeritasTable(HTMLBase):\n",
    "    fields = {'Blazar'                       :0,\n",
    "              'Publication'                  :1,\n",
    "              'VERITAS results page'         :2,\n",
    "              'Ascii file with spectral data':3}\n",
    "\n",
    "    def extract_fields(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        _table = {'Object':[], 'URL':[], 'File':[]}\n",
    "\n",
    "        for i,row in enumerate(self._html.findAll('tr')):\n",
    "            if i==0:\n",
    "                ok = check_table_header(row,self.fields)\n",
    "                if not ok:\n",
    "                    return None\n",
    "                continue\n",
    "            vals = process_row(row,self.fields.copy())\n",
    "            if vals is not None:\n",
    "                src,url,ffile = vals\n",
    "                _table['Object'].append(src)\n",
    "                _table['URL'].append(url)\n",
    "                _table['File'].append(ffile)\n",
    "        return _table\n",
    "    \n",
    "class Web(object):\n",
    "    url   = './'\n",
    "    table = {}\n",
    "    _html = None\n",
    "    HTML_parser = None\n",
    "\n",
    "    def __init__(self,url,HTMLParserClass):\n",
    "        self.url = url\n",
    "        self.HTML_parser = HTMLParserClass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.html)\n",
    "\n",
    "    def get_table(self,table=None):\n",
    "        from bs4 import BeautifulSoup as BS\n",
    "        import urllib2\n",
    "\n",
    "        if table is not None and isinstance(table,dict):\n",
    "            self.table = table\n",
    "            \n",
    "        assert self.url is not None, self.url\n",
    "        assert isinstance(self.url,(str,unicode)), type(self.url)\n",
    "\n",
    "        soup = BS(urllib2.urlopen( self.url ).read(),\"html.parser\")\n",
    "        table = soup.find('table', self.table )\n",
    "        self._html = self.HTML_parser(table)\n",
    "\n",
    "    @property\n",
    "    def html(self):\n",
    "        return self._html.html()\n",
    "    \n",
    "    def get_table_fields(self):\n",
    "        return self._html.extract_fields()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input/Web table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table align=\"center\" border=\"1\" frame=\"vsides\" style=\"border: 1px solid #000000; height: 587px;\" width=\"669\">\n",
       "<tbody>\n",
       "<tr>\n",
       "<td><strong>Blazar</strong></td>\n",
       "<td><strong>Publication</strong></td>\n",
       "<td><strong>VERITAS results page <br/></strong></td>\n",
       "<td><strong>Ascii file with spectral data<br/></strong></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>1ES0229+200   <br/></td>\n",
       "<td><sup>Ap.J. 782, 13 (2014)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/376-1es0229\">results</a></td>\n",
       "<td><a href=\"/documents/1ES0229+200_VERITAS_2009_2012_2014ApJ...782...13A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>1ES0414+009<br/></td>\n",
       "<td><sup>Ap.J. 755, 118 (2012)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/338-1es0414\">results</a></td>\n",
       "<td><a href=\"/documents/1ES0414+009_VERITAS_2008-2011_2012ApJ...755..118A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>1ES0806+524<br/></td>\n",
       "<td><sup>Ap.J. 690, L126 (2009)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/244-1es0806\">results</a></td>\n",
       "<td><a href=\"/documents/1ES0806+524_VERITAS_2006-2008_2009ApJ...690L.126A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>1ES1215+303<br/></td>\n",
       "<td><sup>Ap.J. 779, 92 (2013)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/368-b1215\">results</a></td>\n",
       "<td><a href=\"/documents/1ES1215+303_VERITAS_2008-2012_2013ApJ...779...92A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>1ES1218+304<br/></td>\n",
       "<td><sup>Ap.J. 695, 1370 (2009)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/245-1es1218\">results</a></td>\n",
       "<td><a href=\"/documents/1ES1218+304_VERITAS_2007_2009ApJ...695.1370A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>1ES1218+304<br/></td>\n",
       "<td><sup>Ap.J. 709, L163 (2010)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/236-could-we-put-in-a-journal-reference-here-\">results</a></td>\n",
       "<td><a href=\"/documents/1ES1218+304_VERITAS_2008-2009-2010ApJ...709L.163A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>1ES1959+650 <br/></td>\n",
       "<td><sup>Ap.J. 775, 3 (2013)</sup><br/></td>\n",
       "<td><a href=\"/veritas-science/veritas-results-mainmenu-72/357-1es1959lowflux\">results</a></td>\n",
       "<td><a href=\"/documents/1ES1959_VERITAS_2012.txt\">Ascii data</a><br/></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>1ES2344+514 (high)<br/></td>\n",
       "<td><sup>Ap.J. 738, 169 (2011)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/296-1es2344-mwl\">results</a></td>\n",
       "<td><a href=\"/documents/1ES2344+514_VERITAS_2007_high_2011ApJ...738..169A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>1ES2344+514 (low)<br/></td>\n",
       "<td><sup>Ap.J. 738, 169 (2011)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/296-1es2344-mwl\">results</a></td>\n",
       "<td><a href=\"/documents/1ES2344+514_VERITAS_2007_low_2011ApJ...738..169A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>3C66A (high)<br/></td>\n",
       "<td><sup>Ap.J. 693, L104 (2009)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/252-3c66a\">results</a></td>\n",
       "<td><a href=\"/documents/3C66A_VERITAS_2007-2008_high_2009ApJ...693L.104A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>3C66A (low)<br/></td>\n",
       "<td><sup>Ap.J. 693, L104 (2009)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/252-3c66a\">results</a></td>\n",
       "<td><a href=\"/documents/3C66A_VERITAS_2007-2008_low_2009ApJ...693L.104A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>BLLac<br/></td>\n",
       "<td><sup>Ap.J. 762, 92 (2012)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/349-bllac-rapid-flaring\">results</a></td>\n",
       "<td><a href=\"/documents/BLLacertae_VERITAS_2011_2013ApJ...762...92A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Mrk 501 (high A)<br/></td>\n",
       "<td><sup>Ap.J. 727, 129 (2011)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/269-mrk502-he-emission\">results</a></td>\n",
       "<td><a href=\"/documents/Mkn501_VERITAS_2008_highA_2011ApJ...727..129A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Mrk 501 (high B)<br/></td>\n",
       "<td><sup>Ap.J. 727, 129 (2011)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/269-mrk502-he-emission\">results</a></td>\n",
       "<td><a href=\"/documents/Mkn501_VERITAS_2008_highB_2011ApJ...727..129A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Mrk 501 (high C)<br/></td>\n",
       "<td><sup>Ap.J. 727, 129 (2011)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/269-mrk502-he-emission\">results</a></td>\n",
       "<td><a href=\"/documents/Mkn501_VERITAS_2008_highC_2011ApJ...727..129A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Mrk 501 (low)<br/></td>\n",
       "<td><sup>Ap.J. 727, 129 (2011)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/269-mrk502-he-emission\">results</a></td>\n",
       "<td><a href=\"/documents/Mkn501_VERITAS_2008_low_2011ApJ...727..129A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Mrk 501 (mid)<br/></td>\n",
       "<td><sup>Ap.J. 727, 129 (2011)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/269-mrk502-he-emission\">results</a></td>\n",
       "<td><a href=\"/documents/Mkn501_VERITAS_2008_mid_2011ApJ...727..129A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Mrk 501 (very high)<br/></td>\n",
       "<td><sup>Ap.J. 727, 129 (2011)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/269-mrk502-he-emission\">results</a></td>\n",
       "<td><a href=\"/documents/Mkn501_VERITAS_2008_veryhigh_2011ApJ...727..129A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Mrk 501 (very low)<br/></td>\n",
       "<td><sup>Ap.J. 727, 129 (2011)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/269-mrk502-he-emission\">results</a></td>\n",
       "<td><a href=\"/documents/Mkn501_VERITAS_2008_verylow_2011ApJ...727..129A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>PG1553+113<br/></td>\n",
       "<td><sup> </sup>Ap.J. 799, 7 (2015) <br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/389-pg1553\">results</a></td>\n",
       "<td><a href=\"/documents/PG1553+113_VERITAS_2010-2012_2014arXiv1411.1439A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>PKS1424+240<br/></td>\n",
       "<td><sup>Ap.J. 785, L16 (2014)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/380-pks1424deep\">results</a></td>\n",
       "<td><a href=\"/documents/PKS1424+240_VERITAS_2009_2014ApJ...785L..16A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>PKS1424+240<br/></td>\n",
       "<td><sup>Ap.J. 785, L16 (2014)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/380-pks1424deep\">results</a></td>\n",
       "<td><a href=\"/documents/PKS1424+240_VERITAS_2009_2014ApJ...785L..16A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>PKS1424+240<br/></td>\n",
       "<td><sup>Ap.J. 785, L16 (2014)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/380-pks1424deep\">results</a></td>\n",
       "<td><a href=\"/documents/PKS1424+240_VERITAS_2013_2014ApJ...785L..16A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>RBS0413<br/></td>\n",
       "<td><sup>Ap.J. 750, 94 (2012)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/318-rbs0413\">results</a></td>\n",
       "<td><a href=\"/documents/RBS0413_VERITAS_2008-2009_2012ApJ...750...94A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>RGB0710+591<br/></td>\n",
       "<td><sup>Ap.J. 715, L49 (2010)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/251-rgbj0710591\">results</a></td>\n",
       "<td><a href=\"/documents/RGBJ0710+591_VERITAS_2008-2009_2010ApJ...715L..49A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>RXJ0648.7+1516<br/></td>\n",
       "<td><sup>Ap.J. 742, 127 (2011)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/324-rxj0648\">results</a></td>\n",
       "<td><a href=\"/documents/RXJ0648.7+1516_VERITAS_2010_2011ApJ...742..127A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>VER0521+211<br/></td>\n",
       "<td><sup>Ap.J. 776, 69 (2013)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/364-ver-j0521211\">results</a></td>\n",
       "<td><a href=\"/documents/VERJ0521+211_VERITAS_2009-2010_2013ApJ...776...69A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>WComae<br/></td>\n",
       "<td><sup>Ap.J. 684, L73 (2008)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/150-w-comae\">results</a></td>\n",
       "<td><a href=\"/documents/WComae_VERITAS_2008-01-04_2008ApJ...684L..73A.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Mrk 421<br/></td>\n",
       "<td><sup>Ap.J. 738, 25 (2011)</sup><br/></td>\n",
       "<td><a href=\"http://veritas.sao.arizona.edu/veritas-science/veritas-results-mainmenu-72/291-mrk421-2006-2008\">results</a></td>\n",
       "<td><a href=\"/documents/Mrk421_VERITAS_2011ApJ...738..25.txt\">Ascii data</a></td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web = Web('http://veritas.sao.arizona.edu/veritas-science/veritas-blazar-spectra',HTMLVeritasTable)\n",
    "web.get_table(\n",
    "    table={'style':\"border: 1px solid #000000; height: 587px;\",\n",
    "           'border':\"1\",\n",
    "           'width':\"669\",\n",
    "           'frame':\"vsides\",\n",
    "           'align':\"center\"\n",
    "           }\n",
    ")\n",
    "from IPython.display import HTML\n",
    "HTML(unicode(web.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = web.get_table_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "class Local(pandas.DataFrame):\n",
    "    def __init__(self,table):\n",
    "        super(Local,self).__init__(table)\n",
    "    \n",
    "    def describe(self):\n",
    "        print super(Local,self).describe()\n",
    "        print \"\\n-> Has Nil?\"\n",
    "        hows_nil = self.isnull().any()\n",
    "        print hows_nil\n",
    "        for c in hows_nil.index:\n",
    "            if not hows_nil[c]: continue\n",
    "            print \"\\n-> Indexes where column '{}' is null:\".format(c)\n",
    "            print self[self[c].isnull()].index.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     File  \\\n",
      "count                                                  29   \n",
      "unique                                                 28   \n",
      "top     /documents/PKS1424+240_VERITAS_2009_2014ApJ......   \n",
      "freq                                                    2   \n",
      "\n",
      "             Object  \\\n",
      "count            29   \n",
      "unique           26   \n",
      "top     PKS1424+240   \n",
      "freq              3   \n",
      "\n",
      "                                                      URL  \n",
      "count                                                  29  \n",
      "unique                                                 19  \n",
      "top     http://veritas.sao.arizona.edu/veritas-science...  \n",
      "freq                                                    7  \n",
      "\n",
      "-> Has Nil?\n",
      "File      False\n",
      "Object    False\n",
      "URL       False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "table = Local(f)\n",
    "table.describe()\n",
    "# print table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_dir(_dir,ext):\n",
    "    import os\n",
    "    from glob import glob\n",
    "    if not os.path.exists(_dir):\n",
    "        os.mkdir(_dir)\n",
    "    if os.path.isdir(_dir):\n",
    "        files = glob(os.path.join(_dir,ext))\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "\n",
    "class Download(object):\n",
    "    def __init__(self,outdir,clean=True):\n",
    "        import os\n",
    "        if not os.path.exists(outdir):\n",
    "            os.mkdir(outdir)\n",
    "        self._outdir = outdir\n",
    "        if clean:\n",
    "            self.clean_outdir()\n",
    "        self._md5 = os.path.join(self._outdir,'md5sum')\n",
    "        \n",
    "    def download(self,url):\n",
    "        import wget\n",
    "        filename = wget.download(url,out=self._outdir)\n",
    "        return filename\n",
    "\n",
    "    def clean_outdir(self,ext=\"*\"):\n",
    "        _dir = self._outdir\n",
    "        clean_dir(_dir,ext)\n",
    "                \n",
    "\n",
    "    def create_md5sum_file(self,files_list,_dir=None):\n",
    "        import hashlib\n",
    "        md5txt = os.path.join(_dir,self._md5) if _dir else self._md5\n",
    "        md5 = {}\n",
    "        for f in files_list:\n",
    "#             fname = os.path.join(_dir,f) if _dir else f\n",
    "            fname = f\n",
    "            h = None\n",
    "            with open(fname,'rb') as fp:\n",
    "                h = hashlib.md5(fp.read()).hexdigest()\n",
    "            md5.update({f:h})\n",
    "        with open(md5txt,'w') as fp:\n",
    "            for _file,_hash in md5.iteritems():\n",
    "                fp.write(\"%s    %s\\n\"%(_hash,_file))\n",
    "        return md5\n",
    "\n",
    "    def is_exist_files(self,files_list,_dir=None):\n",
    "        import os\n",
    "        if _dir is None:\n",
    "            _dir = self._outdir\n",
    "        md5_file = self._md5\n",
    "        \n",
    "        # First we see if there is a file list (md5sum) to look for\n",
    "        def check_md5sum(files_list,md5_file):\n",
    "            if os.path.isfile(md5_file):\n",
    "                md5 = read_md5sum_file(md5_file)\n",
    "                md5_files_list = md5.keys()\n",
    "                leng_inters = len(set(md5_files_list).intersection(files_list))\n",
    "                return leng_inters == len(files_list)\n",
    "            # If there is *no* md5-file, return *None*\n",
    "            return None\n",
    "\n",
    "        # Also, check if the files are actually there (inside the dir)\n",
    "        def check_glob(files_list,_dir):\n",
    "            files_ext = '*.txt'\n",
    "            dir_files_list = read_dir_content(_dir,files_ext)\n",
    "            leng_matches = sum(map(lambda v: v in dir_files_list, files_list))\n",
    "            return leng_matches == len(files_list)\n",
    "\n",
    "        md5_check = check_md5sum(files_list,md5_file)\n",
    "        if md5_check in (True,False):\n",
    "            return md5_check\n",
    "        glob_check = check_glob(files_list,_dir)\n",
    "        if glob_check:\n",
    "            self.create_md5sum_file(files_list,_dir=_dir)\n",
    "        return glob_check\n",
    "\n",
    "def read_dir_content(_dir,ext='*'):\n",
    "    from glob import glob\n",
    "    dir_files_list = glob(os.path.join(_dir,ext))\n",
    "    return [ os.path.basename(f) for f in dir_files_list ]\n",
    "\n",
    "def read_md5sum_file(md5txt):\n",
    "    import os\n",
    "    assert os.path.isfile(md5txt)\n",
    "\n",
    "    md5_hashs,md5_files = [],[]\n",
    "    with open(md5txt,'r') as mdf:\n",
    "        for line in mdf.readlines():\n",
    "            _h,_f = line.split(None,1)\n",
    "            md5_hashs.append(_h.strip())\n",
    "            md5_files.append(_f.strip())\n",
    "    md5 = dict(zip(md5_files,md5_hashs))\n",
    "    return md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_dir = 'data/'\n",
    "download_handler = Download(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FITS files do not exist locally. Downloading them...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "_files = table['File'].dropna().apply(lambda f:os.path.join(download_dir,os.path.basename(f)))\n",
    "if download_handler.is_exist_files(_files):\n",
    "    print(\"FITS files exist locally. Passing by download step..\")\n",
    "    _files = _files.apply(lambda f: os.path.join(download_dir,f))\n",
    "else:\n",
    "    print(\"FITS files do not exist locally. Downloading them...\")\n",
    "    furls = veritas_url + table['File']\n",
    "    _files = furls.apply(lambda f: download_handler.download(f))\n",
    "    md5s = download_handler.create_md5sum_file(_files)\n",
    "    del furls\n",
    "\n",
    "table['File'] = _files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 File  \\\n",
      "0   data//1ES0229+200_VERITAS_2009_2012_2014ApJ......   \n",
      "1   data//1ES0414+009_VERITAS_2008-2011_2012ApJ......   \n",
      "2   data//1ES0806+524_VERITAS_2006-2008_2009ApJ......   \n",
      "3   data//1ES1215+303_VERITAS_2008-2012_2013ApJ......   \n",
      "4   data//1ES1218+304_VERITAS_2007_2009ApJ...695.1...   \n",
      "..                                                ...   \n",
      "24  data//RGBJ0710+591_VERITAS_2008-2009_2010ApJ.....   \n",
      "25  data//RXJ0648.7+1516_VERITAS_2010_2011ApJ...74...   \n",
      "26  data//VERJ0521+211_VERITAS_2009-2010_2013ApJ.....   \n",
      "27  data//WComae_VERITAS_2008-01-04_2008ApJ...684L...   \n",
      "28         data//Mrk421_VERITAS_2011ApJ...738..25.txt   \n",
      "\n",
      "            Object  \\\n",
      "0      1ES0229+200   \n",
      "1      1ES0414+009   \n",
      "2      1ES0806+524   \n",
      "3      1ES1215+303   \n",
      "4      1ES1218+304   \n",
      "..             ...   \n",
      "24     RGB0710+591   \n",
      "25  RXJ0648.7+1516   \n",
      "26     VER0521+211   \n",
      "27          WComae   \n",
      "28         Mrk 421   \n",
      "\n",
      "                                                  URL  \n",
      "0   http://veritas.sao.arizona.edu/veritas-science...  \n",
      "1   http://veritas.sao.arizona.edu/veritas-science...  \n",
      "2   http://veritas.sao.arizona.edu/veritas-science...  \n",
      "3   http://veritas.sao.arizona.edu/veritas-science...  \n",
      "4   http://veritas.sao.arizona.edu/veritas-science...  \n",
      "..                                                ...  \n",
      "24  http://veritas.sao.arizona.edu/veritas-science...  \n",
      "25  http://veritas.sao.arizona.edu/veritas-science...  \n",
      "26  http://veritas.sao.arizona.edu/veritas-science...  \n",
      "27  http://veritas.sao.arizona.edu/veritas-science...  \n",
      "28  http://veritas.sao.arizona.edu/veritas-science...  \n",
      "\n",
      "[29 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we can process the fits files themselves.\n",
    "# He start noting that we want the SPECTRUM Data Unit(s)\n",
    "#  available (or not) in the fits files; discard the other DU.\n",
    "# Things we want to do:\n",
    "# - get the OBJECT name\n",
    "# - get the each object position\n",
    "# - get the observation date\n",
    "# - transform the data vectors (x) to frequency(Hz) and (y) to flux(erg/s/cm2)\n",
    "# Then we should follow the following workflow:\n",
    "# - open the fits file\n",
    "# - find the necessary data unit (SPECTRUM)\n",
    "# - open its header\n",
    "#  - get some keywords from the header\n",
    "# - open its data; data here are vectors\n",
    "#  - it can be from 2 to 4 vectors\n",
    "#   - energy\n",
    "#   - flux\n",
    "#   - Denergy\n",
    "#   - Dflux\n",
    "#  - convert the ?energy vectors to 'Hz' units\n",
    "#  - convert the ?flux vectors to 'erg/s/cm2' units\n",
    "\n",
    "# Here we just define the functions we'll need..\n",
    "def resolve_name(name):\n",
    "    from astropy.coordinates import get_icrs_coordinates as get_coords\n",
    "    try:\n",
    "        icrs = get_coords(name)\n",
    "        pos = (icrs.ra.value,icrs.dec.value)\n",
    "    except:\n",
    "        pos = None\n",
    "    return pos\n",
    "\n",
    "def fix_dateobs(date):\n",
    "    try:\n",
    "        dt = str(date).split('-')\n",
    "        y = int(dt[0])\n",
    "    except:\n",
    "        return '1999-01-01'\n",
    "    try:\n",
    "        m = int(dt[1])\n",
    "    except:\n",
    "        m = 1\n",
    "    try:\n",
    "        d = int(dt[2])\n",
    "    except:\n",
    "        d = 1\n",
    "    return '{:4d}-{:02d}-{:02d}'.format(y,m,d)\n",
    "\n",
    "def merge_header_keywords(header_p,header_s):\n",
    "    # Extension's header has the highest priority; keywords there\n",
    "    # should not be overwritten. Relevant keywords are the ones in:\n",
    "    # 'FITS_KEYWORDS'\n",
    "    f_header = {'COMMENT':[]}\n",
    "    _kw = list(set(header_p.keys()).intersection(FITS_KEYWORDS))\n",
    "    for k in _kw:\n",
    "        f_header.update({k : header_p[k]})\n",
    "    if 'COMMENT' in header_p.keys():\n",
    "        f_header['COMMENT'].extend(header_p['COMMENT'])\n",
    "    _kw = list(set(header_s.keys()).intersection(FITS_KEYWORDS))\n",
    "    for k in _kw:\n",
    "        f_header.update({k : header_s[k]})\n",
    "    if 'COMMENT' in header_s.keys():\n",
    "        f_header['COMMENT'].extend(header_s['COMMENT'])\n",
    "    return f_header\n",
    "    \n",
    "def trans_data(table):\n",
    "    import numpy as np\n",
    "    from astropy import units\n",
    "    Unit = units.Unit\n",
    "    \n",
    "    units.set_enabled_equivalencies(units.spectral())\n",
    "    uEn = Unit('Hz')\n",
    "    uFn = Unit('erg s-1 cm-2')\n",
    "    uEc = Unit('TeV')\n",
    "    conv = {Unit('ph TeV s-1 cm-2') : lambda x,y: (x/Unit('ph')).to(uFn),\n",
    "            Unit('ph TeV-1 s-1 cm-2') : lambda x,y: ((y.to(uEc)**2)*(x/Unit('ph'))).to(uFn),\n",
    "            Unit('ph s-1 cm-2') : lambda x,y: None,\n",
    "            Unit('GeV') : lambda x: x.to(uEn, equivalencies=units.spectral())}\n",
    "\n",
    "    vE = table['energy']\n",
    "    uE = vE.unit\n",
    "    vEn = conv[uE](vE)\n",
    "\n",
    "    vF = table['flux']\n",
    "    uF = vF.unit\n",
    "    vFn = conv[uF](vF,vE)\n",
    "\n",
    "    if vFn is None:\n",
    "        print(\"Flux data could not be transformed. Unrecognised units ({})?\".format(uF.to_string()))\n",
    "        return False\n",
    "\n",
    "    def set_null(column,null_expression,new_null_value=-999):\n",
    "        _idx = np.where(null_expression(column))\n",
    "        column[_idx] = new_null_value\n",
    "        column.null = new_null_value\n",
    "        \n",
    "    nullval = -999\n",
    "    new_nullval = nullval\n",
    "    \n",
    "    table['energy'] = vEn\n",
    "    table['energy'].unit = vEn.unit\n",
    "    set_null( table['energy'], lambda x:x==0.0)\n",
    "    table['flux'] = vFn\n",
    "    table['flux'].unit = vFn.unit\n",
    "    set_null( table['flux'], lambda x:x==0.0)\n",
    "    set_null( table['flux'], lambda x:x>0.001)\n",
    "\n",
    "    if 'Denergy' in table.colnames:\n",
    "        vDE = table['Denergy']\n",
    "        uDE = vDE.unit\n",
    "        vDEn = conv[uDE](vDE)\n",
    "        table['energy_error'] = vDEn\n",
    "        table['energy_error'].unit = vDEn.unit\n",
    "        set_null( table['energy_error'], lambda x:x==0.0)\n",
    "        del table['Denergy']\n",
    "    else:\n",
    "        uDE = table['energy'].unit\n",
    "        vDEn = np.asarray([nullval]*len(vE),dtype=int)\n",
    "        table['energy_error'] = vDEn\n",
    "        table['energy_error'].unit = uDE\n",
    "        table['energy_error'].null = nullval\n",
    "\n",
    "    if 'Dflux' in table.colnames:\n",
    "        vDF = table['Dflux']\n",
    "        uDF = vDF.unit\n",
    "        vDFn = conv[uDF](vDF,vE) # Notice we use the energy bin/value of the measurement.\n",
    "        table['flux_error'] = vDFn\n",
    "        table['flux_error'].unit = vDFn.unit\n",
    "        set_null( table['flux_error'], lambda x:x==0.0)\n",
    "        del table['Dflux']\n",
    "    else:\n",
    "        uDF = table['flux'].unit\n",
    "        vDFn = np.asarray([nullval]*len(vE),dtype=int)\n",
    "        table['flux_error'] = vDFn\n",
    "        table['flux_error'].unit = uDF\n",
    "        table['flux_error'].null = nullval\n",
    "\n",
    "    return True\n",
    "\n",
    "# def header_to_dict(header):\n",
    "#     from collections import OrderedDict\n",
    "#     out = OrderedDict()\n",
    "#     for card in header.cards:\n",
    "#         k = card[0]\n",
    "#         v = card[1]\n",
    "#         c = card[2]\n",
    "#         out[k] = v   # 'c' is out for the time being\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def proc_source(filename,source_name,url_results):\n",
    "    \"\"\"\n",
    "    Returns a (plain) list with all valid spectra in it.\n",
    "    \"\"\"\n",
    "    \n",
    "    print \"\\n======================================================================\"\n",
    "    print \"Taking file: \",filename\n",
    "    print \"Source name: \",source_name\n",
    "\n",
    "    # Clean source name\n",
    "    source_name = clean_source_name(source_name)\n",
    "    \n",
    "    # Parse the results page to \n",
    "    # Open the data file, if something goes wrong, return 'None'\n",
    "    data = read_file(filename)\n",
    "    \n",
    "    if not data:\n",
    "        eprint(\"***: File opening failed. Moving on.\")\n",
    "        return None\n",
    "    \n",
    "    # Verify whether what we have from the first (and only) header line is OK\n",
    "    # (basically, check whether units make sense and are compatible to what we want)\n",
    "    is_metadata_ok = verify_metadata(data)\n",
    "    if not is_metadata_ok:\n",
    "        eprint(\"***: Something is wrong with {}:{} metadata.\".format(source_name,filename))\n",
    "        return None\n",
    "    \n",
    "    # Transform out data (wavelength and flux) to what we want\n",
    "    data.transform_data()\n",
    "    \n",
    "    # In VERITAS data files should not exist 'NA' values.\n",
    "    # Which means, 'del_rows' should be zero\n",
    "    del_rows = data.dropna(columns=['energy','flux'],na_value=-999)\n",
    "    if del_rows:\n",
    "        assert False, 'VERITAS data files should not present NA values!'\n",
    "        print \"\\n{} rows eliminated from table: {}.\".format(len(del_rows),del_rows)\n",
    "        if len(spec)==0:\n",
    "            eprint(\"***: Table is empty; not to be writen. Continue to next spectrum.\")\n",
    "            continue\n",
    "    print \"\\nOutput filename: {}\".format(data.suggest_output_filename(output_dir=''))\n",
    "    print \"**********************************************************************\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files processing logfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Taking file:  data//1ES0229+200_VERITAS_2009_2012_2014ApJ...782...13A.txt\n",
      "Source name:  1ES0229+200\n",
      "\n",
      "======================================================================\n",
      "Taking file:  data//1ES0229+200_VERITAS_2009_2012_2014ApJ...782...13A.txt\n",
      "Source name:  1ES0229+200\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "(\"global name 'clean_source_name' is not defined\", u'occurred at index 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-749cd6b2396d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SPECTRUM'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mproc_source\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/chbrandt/.conda/envs/veritas/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[0;32m   4150\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4151\u001b[0m                         \u001b[0mreduce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4152\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4153\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4154\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chbrandt/.conda/envs/veritas/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_apply_standard\u001b[1;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[0;32m   4246\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4247\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4248\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4249\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4250\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-749cd6b2396d>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SPECTRUM'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mproc_source\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-9ba948c74345>\u001b[0m in \u001b[0;36mproc_source\u001b[1;34m(filename, source_name, url_results)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Clean source name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msource_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_source_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0msource_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: (\"global name 'clean_source_name' is not defined\", u'occurred at index 0')"
     ]
    }
   ],
   "source": [
    "table['SPECTRUM'] = table.apply(lambda x: proc_source(x.File,x.Object,x.URL), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_degeneracy(group):\n",
    "    from collections import OrderedDict\n",
    "    row = group.irow(0)\n",
    "    columns = row.to_dict()\n",
    "    specs = row['SPECTRUM']\n",
    "    del columns['SPECTRUM']\n",
    "    tdf = OrderedDict()\n",
    "    tdf['OBJECT'] = []\n",
    "    tdf['RA'] = []\n",
    "    tdf['DEC'] = []\n",
    "    tdf['DATE-OBS'] = []\n",
    "    tdf['SPECTRUM'] = []\n",
    "    tdf['SRCPOS1'] = []\n",
    "    tdf['SRCPOS2'] = []\n",
    "    tdf['EBL_CORR'] = []\n",
    "    cnt = 0\n",
    "    for s in specs:\n",
    "        t = s.retrieve_table()\n",
    "        tdf['RA'].append( t.meta['RA'] )\n",
    "        tdf['DEC'].append( t.meta['DEC'] )\n",
    "        tdf['OBJECT'].append( t.meta['OBJECT'] )\n",
    "        tdf['DATE-OBS'].append( t.meta['DATE-OBS'] )\n",
    "        tdf['SPECTRUM'].append( s )\n",
    "        tdf['SRCPOS1'].append( t.meta['SRCPOS1'] )\n",
    "        tdf['SRCPOS2'].append( t.meta['SRCPOS2'] )\n",
    "        tdf['EBL_CORR'].append( t.meta['EBL_CORR'] )\n",
    "        cnt += 1\n",
    "    for c in columns:\n",
    "        tdf[c] = [ row[c] ] * cnt\n",
    "    return Local( tdf )\n",
    "\n",
    "table_proc = table.dropna().groupby('URL',group_keys=False).apply(fix_degeneracy).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outdir = 'FITS_out/'\n",
    "clean_dir(outdir,'*')\n",
    "\n",
    "for _spec in table_proc.SPECTRUM:\n",
    "    write_to_fits(_spec,outdir)\n",
    "#_bla = table_proc.apply(lambda d:write_to_fits(d.SPECTRUM,outdir),axis=1)\n",
    "#del _bla\n",
    "\n",
    "table_proc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows',200)\n",
    "pd.set_option('display.max_columns',10)\n",
    "pd.set_option('display.width',500)\n",
    "\n",
    "print table_proc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_final = table_proc.dropna()[['OBJECT','RA','DEC','URL','FITS','DATE-OBS']]\n",
    "print table_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
